{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터를 로드하여 데이터프레임으로 변환한다\n",
    "train_df = Loader.load_amazon_reviews('train')\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_amazon_reviews('test')\n",
    "print(f'test_df.shape : {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습셋에서 랜덤으로 2만개만 추출하여 feature 추출에 사용한다\n",
    "dataset = train_df.sample(n=20000, random_state=6)\n",
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 20만개 데이터 샘플에서 review, sentiment 칼럼 값들 추출\n",
    "corpus = dataset['review'].values\n",
    "target = dataset['sentiment'].values\n",
    "print(f'corpus.shape : {corpus.shape}')\n",
    "print(f'target.shape : {target.shape}')\n",
    "\n",
    "# 유효하지 않은 값 제거 (비어있거나 길이가 30 이하인 경우 제거)\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')\n",
    "print(f'corpus_to_seq[0] :')\n",
    "print(corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱되기 전 원본 문서\n",
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_corpus = test_df['review'].values\n",
    "holdout_target = test_df['sentiment'].values\n",
    "print(f'holdout_corpus.shape : {holdout_corpus.shape}')\n",
    "print(f'holdout_target.shape : {holdout_target.shape}')\n",
    "\n",
    "holdout_corpus, holdout_target = remove_empty_docs(holdout_corpus, holdout_target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'holdout_corpus size : {len(holdout_corpus)}')\n",
    "print(f'holdout_target size : {len(holdout_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환 (위에서 생성한 인덱스 사전 그대로 사용)\n",
    "holdout_corpus_to_seq = preprocessor.transform(holdout_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'holdout_corpus_to_seq size : {len(holdout_corpus_to_seq)}')\n",
    "print(f'holdout_corpus_to_seq[0] size : {len(holdout_corpus_to_seq[0])}')\n",
    "print(f'holdout_corpus_to_seq[0] :')\n",
    "print(holdout_corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱된 텍스트 데이터를 GloVe로 임베딩 초기화.\n",
    "# glove.6B.50d.txt에 없는 단어는 OOV..txt에 write한다\n",
    "# word_index는 {'expensive': 2, 'junk': 3, 'this': 4, ...} 형태의 인덱싱 사전\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 사전의 단어 수\n",
    "len(preprocessor.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe로 임베딩 초기화된 행렬. 벡터 개수는 word_index 인덱스 사전의 단어 + 2, 차원 수는 50이다\n",
    "initial_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 기반 문서 분류 모델 인스턴스 생성. 위에서 GloVe로 만든 임베딩 행렬을 임베딩 초깃값으로 사용한다\n",
    "naver_review_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                                    word_index=preprocessor.word_index,\n",
    "                                    num_sentences=Preprocess.NUM_SENTENCES,\n",
    "                                    embedding_weights=initial_embeddings,\n",
    "                                    conv_activation='tanh',\n",
    "                                    hidden_dims=64,\n",
    "                                    input_dropout=0.40,\n",
    "                                    hidden_gaussian_noise_sd=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'amazonreviews')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'amazonreviews'))\n",
    "\n",
    "# 학습 파라미터 저장 클래스\n",
    "train_params = TrainingParameters('model_with_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR + '/amazonreviews/naver_model.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR + '/amazonreviews/naver_model.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR + '/amazonreviews/naver_model_meta.json',\n",
    "                                  num_epochs=1000)\n",
    "\n",
    "# 모델 컴파일\n",
    "naver_review_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                                       optimizer=train_params.optimizer,\n",
    "                                                       metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 자동저장 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 모델에 입력할 학습데이터, 테스트데이터 (인덱스 값들의 시퀀스로 변환된 값)\n",
    "x_train = np.array(corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "x_test = np.array(holdout_corpus_to_seq)\n",
    "y_test = np.array(holdout_target)\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')\n",
    "\n",
    "# 모델 훈련 시작\n",
    "history = naver_review_model.get_classification_model().fit(x_train,\n",
    "                                                   y_train, \n",
    "                                                   batch_size=train_params.batch_size, \n",
    "                                                   epochs=train_params.num_epochs,  # 35 epochs\n",
    "                                                   verbose=2,\n",
    "                                                   validation_split=train_params.validation_split, # 5%\n",
    "                                                   callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "naver_review_model._save_model(train_params.model_hyper_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 - 테스트 데이터셋으로 수행\n",
    "naver_review_model.get_classification_model().evaluate(x_test,\n",
    "                                                        y_test, \n",
    "                                                        train_params.batch_size*10,\n",
    "                                                        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_embeddings = naver_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "\n",
    "embd_change = {}\n",
    "for word, i in preprocessor.word_index.items():\n",
    "    # Frobenium norm (Euclidean norm) 계\n",
    "    embd_change[word] = np.linalg.norm(initial_embeddings[i]-learned_embeddings[i])\n",
    "embd_change = sorted(embd_change.items(), key=lambda x: x[1], reverse=True)\n",
    "embd_change[0:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
