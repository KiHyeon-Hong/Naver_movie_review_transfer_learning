{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'imdb')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'imdb'))\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "train_params = TrainingParameters('imdb_transfer_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR+ '/imdb/naver_transfer_model.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR+ '/imdb/naver_transfer_model.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR+ '/imdb/naver_transfer_model.json',\n",
    "                                  num_epochs=1000,\n",
    "                                  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = Loader.load_imdb_data(directory = 'train')\n",
    "# train_df = train_df.sample(frac=0.05, random_state = train_params.seed)\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_imdb_data(directory = 'test')\n",
    "print(f'test_df.shape : {test_df.shape}')\n",
    "\n",
    "# 텍스트 데이터, 레이블 추출\n",
    "corpus = train_df['review'].tolist()\n",
    "target = train_df['sentiment'].tolist()\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocess.NUM_SENTENCES = 20\n",
    "\n",
    "# 학습셋을 인덱스 시퀀스로 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환\n",
    "test_corpus = test_df['review'].tolist()\n",
    "test_target = test_df['sentiment'].tolist()\n",
    "test_corpus, test_target = remove_empty_docs(test_corpus, test_target)\n",
    "test_corpus_to_seq = preprocessor.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test_corpus_to_seq size : {len(test_corpus_to_seq)}')\n",
    "print(f'test_corpus_to_seq[0] size : {len(test_corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습셋, 테스트셋 준비\n",
    "x_train = np.array(corpus_to_seq)\n",
    "x_test = np.array(test_corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "y_test = np.array(test_target)\n",
    "\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe 임베딩 초기화 - glove.6B.50d.txt pretrained 벡터 사용\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n",
    "print(f'initial_embeddings.shape : {initial_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_embeddings[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 로드\n",
    "model_json_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.json')\n",
    "amazon_review_model = DocumentModel.load_model(model_json_path)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model_hdf5_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.hdf5')\n",
    "amazon_review_model.load_model_weights(model_hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 임베딩 레이어 추출\n",
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "print(f'learned_embeddings size : {len(learned_embeddings)}')\n",
    "\n",
    "# 기존 GloVe 모델을 학습된 임베딩 행렬로 업데이트한다\n",
    "glove.update_embeddings(preprocessor.word_index, \n",
    "                        np.array(learned_embeddings), \n",
    "                        amazon_review_model.word_index)\n",
    "\n",
    "# 업데이트된 임베딩을 얻는다\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                           word_index = preprocessor.word_index,\n",
    "                           num_sentences=Preprocess.NUM_SENTENCES,     \n",
    "                           embedding_weights=initial_embeddings,\n",
    "                           embedding_regularizer_l2 = 0.0,\n",
    "                           conv_activation = 'tanh',\n",
    "                           train_embedding = True,   # 임베딩 레이어의 가중치 학습함\n",
    "                           learn_word_conv = False,  # 단어 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           learn_sent_conv = False,  # 문장 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           hidden_dims=64,                                        \n",
    "                           input_dropout=0.1, \n",
    "                           hidden_layer_kernel_regularizer=0.01,\n",
    "                           final_layer_kernel_regularizer=0.01)\n",
    "\n",
    "for l_name in ['word_conv','sentence_conv','hidden_0', 'final']:\n",
    "    new_weights = amazon_review_model.get_classification_model().get_layer(l_name).get_weights()\n",
    "    naver_model.get_classification_model().get_layer(l_name).set_weights(weights=new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 모델 컴파일              \n",
    "naver_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                              optimizer='rmsprop',\n",
    "                                              metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                                verbose=1,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 학습 시작\n",
    "history = naver_model.get_classification_model().fit(x_train, \n",
    "                                          y_train, \n",
    "                                          batch_size=train_params.batch_size,\n",
    "                                          epochs=train_params.num_epochs,\n",
    "                                          verbose=2,\n",
    "                                          validation_split=0.01,\n",
    "                                          callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "naver_model._save_model(train_params.model_hyper_parameters)\n",
    "train_params.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "naver_model.get_classification_model().evaluate(x_test, \n",
    "                                               y_test, \n",
    "                                               batch_size=train_params.batch_size*10,\n",
    "                                               verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_embeddings = naver_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "\n",
    "embd_change = {}\n",
    "for word, i in preprocessor.word_index.items():\n",
    "    # Frobenium norm (Euclidean norm) 계\n",
    "    embd_change[word] = np.linalg.norm(initial_embeddings[i]-learned_embeddings[i])\n",
    "embd_change = sorted(embd_change.items(), key=lambda x: x[1], reverse=True)\n",
    "embd_change[0:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
